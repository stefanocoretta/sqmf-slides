<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistics and Quantitative Methods (S1)</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr Stefano Coretta" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/freezeframe/freezeframe.min.js"></script>
    <script src="libs/xaringanExtra-freezeframe/freezeframe-init.js"></script>
    <script id="xaringanExtra-freezeframe-options" type="application/json">{"selector":"img[src$=\"gif\"]","trigger":"click","overlay":false,"responsive":true,"warnings":true}</script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Statistics and Quantitative Methods (S1)
]
.subtitle[
## Week 5
]
.author[
### Dr Stefano Coretta
]
.institute[
### University of Edinburgh
]
.date[
### 2022/10/08
]

---






class: center middle inverse

# SUMMARY

---

# Summary

* The simplest .orange[**linear model**] is a straight line.

`$$y = \beta_0 + \beta_1 x$$`

* We want to estimate the `\(\beta_n\)` .orange[**coefficients**].

`$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$$`

* Categorical predictors are .orange[coded as numbers].

* The default coding system ("treatment contrasts") sets the intercept as the mean of the first level (the "reference level").

* The other levels of the categorical predictor are compared to the reference level.


---

class: center middle

![:scale 30%](../../img/charlesdeluvio-D44HIk-qsvI-unsplash.jpg)

???

Photo by &lt;a href="https://unsplash.com/@charlesdeluvio?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"&gt;charlesdeluvio&lt;/a&gt; on &lt;a href="https://unsplash.com/s/photos/sad-puppy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"&gt;Unsplash&lt;/a&gt;

---

layout: true

# Exercise 1

---


```
## 
## Call:
## lm(formula = articulation_rate ~ attitude + musicstudent, data = polite)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2246 -0.6739 -0.1175  0.6190  5.9630 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)       7.0522     0.1219  57.874  &lt; 2e-16
## attitudepol      -0.4088     0.1511  -2.705  0.00737
## musicstudentyes  -0.4470     0.1561  -2.864  0.00459
## 
## Residual standard error: 1.131 on 221 degrees of freedom
## Multiple R-squared:  0.06561,	Adjusted R-squared:  0.05715 
## F-statistic: 7.759 on 2 and 221 DF,  p-value: 0.0005538
```

---

&lt;img src="index_files/figure-html/art-rate-plot-1.png" height="500px" style="display: block; margin: auto;" /&gt;

---

layout: false
layout: true

# Exercise 2

---


```
## 
## Call:
## lm(formula = f0mn ~ attitude + months_ger + gender, data = polite)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -121.994  -26.368   -5.904   20.204  163.443 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  259.52250    5.07675  51.120   &lt;2e-16
## attitudepol  -14.71441    5.31042  -2.771   0.0061
## months_ger    -0.07929    0.04427  -1.791   0.0747
## genderM     -119.08249    5.61931 -21.192   &lt;2e-16
## 
## Residual standard error: 38.65 on 208 degrees of freedom
##   (12 observations deleted due to missingness)
## Multiple R-squared:  0.6965,	Adjusted R-squared:  0.6921 
## F-statistic: 159.1 on 3 and 208 DF,  p-value: &lt; 2.2e-16
```

---

&lt;img src="index_files/figure-html/f0-plot-1.png" height="500px" style="display: block; margin: auto;" /&gt;

---

layout: false
class: center middle

![:scale 80%](../../img/joe-caione-qO-PIF84Vxg-unsplash.jpg)

???

Photo by &lt;a href="https://unsplash.com/@joeyc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"&gt;Joe Caione&lt;/a&gt; on &lt;a href="https://unsplash.com/s/photos/happy-puppy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"&gt;Unsplash&lt;/a&gt;

---

layout: true

# Shallow Morphological Processing

---

- English L1 and L2 speakers (L2 speakers are native speakers of Cantonese).

- Lexical decision task (Word vs Non-Word).

- Target: *unkindness* ([[un]-[kind]]-ness).

- Primes: *prolong* (Unrelated), *unkind* (Constituent), *kindness* (Non-Constituent).

---


```
## # A tibble: 1,950 × 3
##    ID    accuracy  Relation_type 
##    &lt;chr&gt; &lt;fct&gt;     &lt;fct&gt;         
##  1 L1_01 correct   Unrelated     
##  2 L1_01 correct   Constituent   
##  3 L1_01 correct   Unrelated     
##  4 L1_01 correct   Constituent   
##  5 L1_01 incorrect Unrelated     
##  6 L1_01 correct   Unrelated     
##  7 L1_01 correct   Constituent   
##  8 L1_01 correct   NonConstituent
##  9 L1_01 correct   NonConstituent
## 10 L1_01 correct   Constituent   
## # … with 1,940 more rows
```

---

layout: false

# This doesn't work!


```r
shallow_lm &lt;- lm(accuracy ~ Word_Nonword, data = shallow)
```

```
Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
    contrasts can be applied only to factors with 2 or more levels
```

--

&lt;br&gt;

What is the difference between this model and the models we have run so far?

(**NOTE:** the error message is absolutely NOT helpful!)

--

&lt;br&gt;

**Hint:** Compare the the outcome variables.

---

class: center middle inverse

# PROBABILITY DISTRIBUTIONS

---

class: middle

.pull-left[
![](../../img/06-rainy-umbrella-humid.svg)
]

.pull-right[
![](../../img/01-sunny.svg)
]

???

We are faced every day with probabilities. Just think about the weather forecast.

We say things like t"here is a 70% prob that it will rain today". In this sense, probability is the probability of an event occurring.

But what about more complex situations that are not a flip-of-coin kinda situation? For example what about rolling two dice?

Here is where probability distributions come in.

---

# Grubabilities

&amp;nbsp;

.center[
![:scale 90%](../../img/grubabilities.png)
]

???

A probability distribution is a list of values and their corresponding probability.

---

# Discrete and continuous

.center[
![:scale 80%](../../img/discr-cont-probs.png)
]

???

Remember we talked about continuous and discrete variables?

This distinction is helpful not only when deciding which type of plot to use, but also which type of linear model to use!

Or more specifically, which probability distribution to use for the outcome variable. This week's classes will be about this!

Depending on the nature of the values a variable can take, there are 2 types of probs.



---

# Discrete probability distributions

.center[
![:scale 60%](../../img/dice.png)
]

???


A discrete probability distributions is like counting how many ways you can get a particular value.

For example, if you roll a white and a black dice, there are 3 ways to get a 4 or a 10, but 6 ways to get a 7.

---

layout: true

# Continuous probability distributions

---

&lt;img src="index_files/figure-html/cont-p-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

With continuous probabilities we cannot make a list of all the possible values (0.0, 0.00, 0.000, 0.0001...), because there is an infinite number of possible values. So we cannot assign a probability to a specific value.

Instead, we assign probabilities to a range of values.

---

&lt;img src="index_files/figure-html/cont-p-2-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

In this example, we want to know the probability of observing an f0 value between 0 and 160 Hz, assuming the probability distribution represented in the graph.

We simply calculate the area under the curve between those two values (note that the total area under the curve is 1).

The probability of f0 being less than 160 Hz is 0.212.

---

&lt;img src="index_files/figure-html/cont-p-3-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

The probability of f0 being greater than 220 Hz is 0.345.

---

&lt;img src="index_files/figure-html/cont-p-4-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

The probability of f0 being between 120 and 210 Hz is 0.524

---

But how do we describe probability distributions in a succint way?

We can't make a list of all values and probabilities, especially for continuous probabilities.

--

.bg-washed-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[

Instead, we specify the value of the **parameters that describe the distribution**.

]

---

layout: false
class: center middle

.f1.link.dim.br3.ph3.pv2.mb2.dib.white.bg-purple[
[.white[Web App: Probability distributions]](https://seeing-theory.brown.edu/probability-distributions/index.html#section2)
]

&lt;!-- &lt;iframe src="https://seeing-theory.brown.edu/probability-distributions/index.html#section2" style="border:none;" width="100%" height="100%"&gt; --&gt;

---
class: middle

&lt;span style="font-size:3.5em;"&gt;$$y_i \sim Normal(\mu, \sigma)$$&lt;/span&gt;

???

Let's look at some formulae.

This is the formula of a variable `\(y_1\)` that is distributed according to (~) a Normal probability distribution.

As we have seen in the example above, a Normal distribution can be described with two parameters: the mean and the standard deviation.

---

class: middle

&lt;span style="font-size:3.5em;"&gt;$$\text{f0}_i \sim Normal(200, 50)$$&lt;/span&gt;

???

Remember the example above of a Gaussian/Normal distribution of f0?

We can describe that distribution with this formula (much easier than listing all the values and their probability).

---

class: center middle inverse

# Think about the probability distribution of the outcome variable

---

# Common probability distributions


.pull-left[
**Continuous outcome variable**

* The variable can take on *any positive and negative real number, including 0*: **Gaussian** (aka normal) distribution.

    * There are very few truly Gaussian variables, although in some cases one can speak of "approximate" or "assumed" normality.
    
    * This distribution family is fitted by default in `lm(...)`.
]

--

.pull-right[
**Discrete outcome variable**

* The variable is *dichotomous*, i.e. it can take one of two levels: **Bernoulli** distribution.
  * Categorical outcome variables like yes/no, correct/incorrect, voiced/voiceless, follow this distribution.
  
  * This family is fitted when you run `glm(..., family = binomial)`, aka "logistic regression" or "binomial regression".

* The variable is *counts*: **Poisson** distribution.
  * Counts of words, segments, gestures, f0 peaks, ...
  
  * This family can be fitted with `glm(..., family = poisson)`.
]

???

Note that `glm()` stands for Generalised Linear Model.

It's called "generalised" because the maths behind it generalises the use of linear models with the Gaussian distribution family to other distribution families.

But from a practical point of view, these are all linear models, whether you fit them with `lm()` or `glm()`.

---

# So far we used Gaussian distributions

`lm()` uses a Gaussian distribution by default.

--

But most variables are not Gaussian (in fact, there aren't any truly Gaussian variables, so much for the "normal" distribution).

---

class: center middle inverse

# DICHOTOMOUS VARIABLES: ACCURACY

---

layout: true

# Dichotomous variables: Accuracy

---


```
## # A tibble: 1,950 × 3
##    ID    accuracy  Relation_type 
##    &lt;chr&gt; &lt;fct&gt;     &lt;fct&gt;         
##  1 L1_01 correct   Unrelated     
##  2 L1_01 correct   Constituent   
##  3 L1_01 correct   Unrelated     
##  4 L1_01 correct   Constituent   
##  5 L1_01 incorrect Unrelated     
##  6 L1_01 correct   Unrelated     
##  7 L1_01 correct   Constituent   
##  8 L1_01 correct   NonConstituent
##  9 L1_01 correct   NonConstituent
## 10 L1_01 correct   Constituent   
## # … with 1,940 more rows
```

---

&lt;img src="index_files/figure-html/shallow-plot-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

You can clearly see that `"correct"` is much more frequent than `"incorrect"`.

In other words, the probability of getting `"correct"` is greater than the probability of getting `"incorrect"`.

---

&lt;img src="index_files/figure-html/shallow-plot-1-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

What happens if we separate by `Releation_type`?

You can still see that `"correct"` is more frequent than `"incorrect"` in all relation types, but since each type has a different number of observations it is not easy to compare *across* types.

---

&lt;img src="index_files/figure-html/shallow-plot-2-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

In this plot, we use `position = "fill"` to plot *proportions* rather than raw counts.

Now you can see that, proportionally, there are more correct responses in `Constituent` vs `NonConstituent` and that `Unrelated` is sorta mid-way through the other two.

---

.bg-washed-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[

When the outcome variable is **dichotomous**, we need to estimate the **probability** of getting either level in the variable.

]

--

In the `shallow` data:

- `accuracy` is dichotomous with levels `"incorrect"` and `"correct"`
- `Relation_type` is discrete with three levels: `Unrelated`, `Constituent`, `NonConstituent`.

So, we want to know (i.e. estimate) the probability of getting a `"correct"` response depending on `Relation_type`.

We need to use `glm()` and `family = binomial()`!

---


```
## 
## Call:
## glm(formula = accuracy ~ Relation_type, family = binomial(), 
##     data = shallow)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0269   0.5238   0.5238   0.6438   0.7518  
## 
## Coefficients:
##                             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)                   1.4684     0.0918  15.997  &lt; 2e-16
## Relation_typeConstituent      0.4485     0.1411   3.179  0.00148
## Relation_typeNonConstituent  -0.3492     0.1492  -2.341  0.01921
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1810.9  on 1949  degrees of freedom
## Residual deviance: 1784.8  on 1947  degrees of freedom
## AIC: 1790.8
## 
## Number of Fisher Scoring iterations: 4
```

???

Here's the summary of `shallow_lm_1`.

Do you notice something weird?

---

layout: false
class: middle center inverse

# What is the unit of the estimates?!

???

Should be probabilities, because we are estimating probabilities.

But those cannot be probabilities, because probabilities are between 0 and 1.

---

class: middle center

# Probabilities as log-odds

???

Linear models cannot work with probabilities! 

So we need to transform probabilities into something the model can work with.

And that something is log-odds!

---

layout: true

# Probabilities as log-odds

---

&lt;img src="index_files/figure-html/p-log-odds-1.png" height="500px" style="display: block; margin: auto;" /&gt;

---

To transform log-odds to probabilities you can use the `plogis()` function!


```r
# What is the probability at -1 log-odds?
plogis(-1)
```

```
## [1] 0.2689414
```

```r
# What is the probability at 0 log-odds?
plogis(0)
```

```
## [1] 0.5
```

```r
# What is the probability at 1 log-odds?
plogis(1)
```

```
## [1] 0.7310586
```

--

&lt;br&gt;

Now try different numbers with `plogis()`.

---

layout: false
layout: true

# Dichotomous variables: Accuracy

---


```
## 
## Call:
## glm(formula = accuracy ~ Relation_type, family = binomial(), 
##     data = shallow)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0269   0.5238   0.5238   0.6438   0.7518  
## 
## Coefficients:
##                             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)                   1.4684     0.0918  15.997  &lt; 2e-16
## Relation_typeConstituent      0.4485     0.1411   3.179  0.00148
## Relation_typeNonConstituent  -0.3492     0.1492  -2.341  0.01921
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1810.9  on 1949  degrees of freedom
## Residual deviance: 1784.8  on 1947  degrees of freedom
## AIC: 1790.8
## 
## Number of Fisher Scoring iterations: 4
```

---


```
## # A tibble: 3 × 5
##   term                        estimate std.error statistic  p.value
##   &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)                    1.47     0.0918     16.0  1.35e-57
## 2 Relation_typeConstituent       0.448    0.141       3.18 1.48e- 3
## 3 Relation_typeNonConstituent   -0.349    0.149      -2.34 1.92e- 2
```

--

- `Intercept`
  
  - log-odds = 1.4684
  - `plogis(1.4684)` = 0.81
  - i.e. 81% probability of getting `"correct"` *when* `Relation_type` is `"Unrelated"`.

--

- What about `Relation_typeConstituent` and `Relation_typeNonConstituent`?

&lt;br&gt;

.f3[As with the estimates of discrete predictors in `lm()` these tell you the difference between `Intercept` and the predictor level, **but in log-odds** rather than probabilities.]

---


```
## # A tibble: 3 × 5
##   term                        estimate std.error statistic  p.value
##   &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)                    1.47     0.0918     16.0  1.35e-57
## 2 Relation_typeConstituent       0.448    0.141       3.18 1.48e- 3
## 3 Relation_typeNonConstituent   -0.349    0.149      -2.34 1.92e- 2
```

We need to **add** the estimate to the intercept to calculate probabilities!

- `Relation_typeConstituent`

  - log-odds = 0.4485.
  - `plogis(1.4684 + 0.4485)` = 0.87
  - i.e. when `Relation_type` is `Constituent`, we go from 81% to 87% probability of getting `"correct"`.
  - In other words, there is a probability increase of about 6%.
  
--

- `Relation_typeNonConstituent`

  - log-odds = -0.3492.
  - `plogis(1.4684 + -0.3492)` = 0.75
  - i.e. when `Relation_type` is `NonConstituent`, we go from 81% to 75% probability of getting `"correct"`.
  - In other words, there is a probability decrease of about 6%.

---


```r
ggpredict(shallow_lm_1, terms = "Relation_type")
```

```
## # Predicted probabilities of accuracy
## 
## Relation_type  | Predicted |       95% CI
## -----------------------------------------
## Unrelated      |      0.81 | [0.78, 0.84]
## Constituent    |      0.87 | [0.85, 0.89]
## NonConstituent |      0.75 | [0.71, 0.79]
```

---

&lt;img src="index_files/figure-html/shallow-lm-1-plot-1.png" height="500px" style="display: block; margin: auto;" /&gt;


---


```
## 
## Call:
## glm(formula = accuracy ~ Relation_type + Group, family = binomial(), 
##     data = shallow)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0946   0.4863   0.5527   0.6784   0.7909  
## 
## Coefficients:
##                             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)                   1.6259     0.1173  13.855  &lt; 2e-16
## Relation_typeConstituent      0.4495     0.1412   3.183  0.00146
## Relation_typeNonConstituent  -0.3503     0.1494  -2.345  0.01903
## GroupL2                      -0.2738     0.1224  -2.237  0.02526
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1810.9  on 1949  degrees of freedom
## Residual deviance: 1779.7  on 1946  degrees of freedom
## AIC: 1787.7
## 
## Number of Fisher Scoring iterations: 4
```


---


```
## # Predicted probabilities of accuracy
## 
## # Group = L1
## 
## Relation_type  | Predicted |       95% CI
## -----------------------------------------
## Unrelated      |      0.84 | [0.80, 0.86]
## Constituent    |      0.89 | [0.86, 0.91]
## NonConstituent |      0.78 | [0.73, 0.82]
## 
## # Group = L2
## 
## Relation_type  | Predicted |       95% CI
## -----------------------------------------
## Unrelated      |      0.79 | [0.76, 0.83]
## Constituent    |      0.86 | [0.83, 0.88]
## NonConstituent |      0.73 | [0.68, 0.78]
```

---

&lt;img src="index_files/figure-html/shallow-lm-2-pred-1.png" height="500px" style="display: block; margin: auto;" /&gt;

---

layout: false
class: center middle inverse

# COUNTS: CONTINGENT TALKS

---

layout: true

# Counts: Contingent talks

---


```
## # A tibble: 1,620 × 4
##    dyad  background count    ct
##    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;
##  1 b01   Bengali        5     1
##  2 b01   Bengali        0     0
##  3 b01   Bengali        0     0
##  4 b01   Bengali        0     0
##  5 b01   Bengali        0     0
##  6 b01   Bengali        0     0
##  7 b01   Bengali        0     0
##  8 b01   Bengali        0     0
##  9 b01   Bengali        0     0
## 10 b01   Bengali        8     3
## # … with 1,610 more rows
```


---

&lt;img src="index_files/figure-html/gest-plot-1.png" height="500px" style="display: block; margin: auto;" /&gt;


---


```
## 
## Call:
## glm(formula = ct ~ count, family = poisson(), data = gestures)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.4755  -0.5329  -0.5329  -0.5329   6.0319  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -1.951931   0.062940  -31.01   &lt;2e-16
## count        0.133939   0.002916   45.94   &lt;2e-16
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 2265.1  on 1592  degrees of freedom
## Residual deviance: 1266.7  on 1591  degrees of freedom
##   (27 observations deleted due to missingness)
## AIC: 1683.4
## 
## Number of Fisher Scoring iterations: 6
```

---

layout: false
class: middle center inverse

# What is the unit of the estimates?!

???

Should be counts, because we are estimating number of contingent talks.

But those cannot be counts, because counts are discrete and cannot be negative.

---

class: middle center

# Counts as log-odds

???

Linear models don't work well with counts (because they are discrete and cannot be negative)!

So we need to transform counts into something the model can work with.

And that something is (again) log-odds! How convenient!

---

layout: true

# Counts as log-odds

---

To transform log-odds into counts, you can use the `exp()` function.


```r
exp(-1)
```

```
## [1] 0.3678794
```

```r
exp(0)
```

```
## [1] 1
```

```r
exp(1)
```

```
## [1] 2.718282
```

--

&lt;br&gt;

Note that `log()` is the inverse of `exp()`. For example: `log(1)` = 0 and `exp(0)` = 1.

---

layout: false
layout: true

# Counts: Contingent talks

---


```
## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   -1.95    0.0629      -31.0 3.65e-211
## 2 count          0.134   0.00292      45.9 0
```

--

- `Intercept`
  
  - log-odds = -1.9519
  - `exp(-1.9519)` = 0.14.
  - i.e.*when* gesture `count` is `0`, there are between 0 and 1 contingent talks.

--

- What about gesture `count`?

&lt;br&gt;

.f4[As with the estimates of continuous predictors in `lm()` these tell you the difference in `Intercept` when `count` goes from 0 to 1, **but in log-odds** rather than counts.]

--

.f3[When dealing with counts, we normally talk about effects as **rate of change**, aka **odd ratios**.]

???

So we need to transform log-odds into odd ratios (aka simply odds).

How do we do that? Easy! We use the `exp()` function.

So `exp()` converts log-odds into counts and into odd ratios.

---

&lt;img src="index_files/figure-html/log-odds-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

When the odds are 1, then it means that there is no change.

Think about this this way:

If you start off with £5 and every day your savings change by a factor of 1, then every day you will still have those £5.
Because `5 * 1 = 5`.

But if every day your savings increase by a factor of 1.5, then after the first day you have £7.5 (`5 * 1.5 = 7.5`), after the second day you have £11.25 (`7.5 * 1.5 = 11.25`), and so on.

You see that the rate of change is always the same (1.5, or 50%), but the absolute change depends on the day's savings:

- first day: +2.5
- second day: +3.75
- third day: +5.62
- ...

---


```
## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   -1.95    0.0629      -31.0 3.65e-211
## 2 count          0.134   0.00292      45.9 0
```

- `count`

  - log-odds = 0.133939.
  - `exp(0.133939)` = 1.14
  - i.e. for each unit increase of `count`, coontingent talks increase by a factor of 1.14.
  - In other words, for every one extra gesture, there is a 14% increase in contingent talks.



---

&lt;img src="index_files/figure-html/ct-2-plot-1.png" height="500px" style="display: block; margin: auto;" /&gt;


---


```
## 
## Call:
## glm(formula = ct ~ count + background, family = poisson(), data = gestures)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.6725  -0.5924  -0.5744  -0.4268   5.8259  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)       -2.39622    0.12374 -19.365  &lt; 2e-16
## count              0.13032    0.00325  40.097  &lt; 2e-16
## backgroundChinese  0.65593    0.14901   4.402 1.07e-05
## backgroundEnglish  0.59421    0.15219   3.904 9.45e-05
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 2265.1  on 1592  degrees of freedom
## Residual deviance: 1243.6  on 1589  degrees of freedom
##   (27 observations deleted due to missingness)
## AIC: 1664.2
## 
## Number of Fisher Scoring iterations: 6
```

---


```r
tidy(ct_lm_3, exponentiate = FALSE)
```

```
## # A tibble: 4 × 5
##   term              estimate std.error statistic  p.value
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)         -2.40    0.124      -19.4  1.52e-83
## 2 count                0.130   0.00325     40.1  0       
## 3 backgroundChinese    0.656   0.149        4.40 1.07e- 5
## 4 backgroundEnglish    0.594   0.152        3.90 9.45e- 5
```

---


```r
tidy(ct_lm_3, exponentiate = TRUE)
```

```
## # A tibble: 4 × 5
##   term              estimate std.error statistic  p.value
##   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)         0.0911   0.124      -19.4  1.52e-83
## 2 count               1.14     0.00325     40.1  0       
## 3 backgroundChinese   1.93     0.149        4.40 1.07e- 5
## 4 backgroundEnglish   1.81     0.152        3.90 9.45e- 5
```


---

&lt;img src="index_files/figure-html/ct-3-plot-1.png" height="500px" style="display: block; margin: auto;" /&gt;

???

- 10 mo infants perform on average 10 iconic gestures per day. At 11 mo, the number of gestures increases by a factor of 1.6. At 12 mo, there is a further increase by a factor of 1.2.

--

  - Calculate the average number of gestures per day at 12 months based on the 10 month average (10 gestures).

--

- The average number of errors L2 learners make decreases by a factor of 0.2 every year and a half.

--

  - Calculate the average number of errors after 6 years, assuming 265 errors at year 1.

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="../../assets/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
