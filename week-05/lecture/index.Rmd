---
title: "Statistics and Quantitative Methods (S1)"
subtitle: "Week 5"
author: "Dr Stefano Coretta"
institute: "University of Edinburgh"
date: "2022/10/08"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css:
      - xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "../../assets/macros.js"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, dpi = 300, out.height = "500px", fig.align = "center")
knitr::opts_knit$set(root.dir = here::here())
options(htmltools.dir.version = FALSE)
library(tidyverse)
theme_set(theme_light())
library(xaringanExtra)
use_xaringan_extra(c("panelset", "tachyons", "freezeframe"))
library(ggeffects)
options(show.signif.stars = FALSE)
library(broom.mixed)
library(sqmf)
data("polite")
polite <- polite %>%
  mutate(
    attitude = factor(attitude, levels = c("inf", "pol")),
    musicstudent = factor(musicstudent, levels = c("no", "yes"))
  )
data("shallow")
data("gestures")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  text_font_google = google_font("Lato", "400", "700", "400i", "700i"),
  header_font_google = google_font("Montserrat", "400", "700", "400i", "700i"),
  code_font_google = google_font("Source Code Pro", "400", "700")
)
```

class: center middle inverse

# SUMMARY

---

# Summary

* The simplest .orange[**linear model**] is a straight line.

$$y = \beta_0 + \beta_1 x$$

* We want to estimate the $\beta_n$ .orange[**coefficients**].

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$$

* Categorical predictors are .orange[coded as numbers].

* The default coding system ("treatment contrasts") sets the intercept as the mean of the first level (the "reference level").

* The other levels of the categorical predictor are compared to the reference level.


---

class: center middle

![:scale 30%](../../img/charlesdeluvio-D44HIk-qsvI-unsplash.jpg)

???

Photo by <a href="https://unsplash.com/@charlesdeluvio?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">charlesdeluvio</a> on <a href="https://unsplash.com/s/photos/sad-puppy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>

---

layout: true

# Exercise 1

---

```{r art-rate}
art_lm_1 <- lm(articulation_rate ~ attitude + musicstudent, data = polite)

summary(art_lm_1)
```

---

```{r art-rate-plot}
ggpredict(art_lm_1, terms = c("attitude", "musicstudent")) %>%
  plot() +
  theme(text = element_text(size = 20))
```

---

layout: false
layout: true

# Exercise 2

---

```{r f0}
f0_lm_1 <- lm(f0mn ~ attitude + months_ger + gender, data = polite)

summary(f0_lm_1)
```

---

```{r f0-plot}
ggpredict(f0_lm_1, terms = c("attitude", "months_ger [0, 24, 48]", "gender"), condition = ) %>%
  plot() +
  theme(text = element_text(size = 20))
```

---

layout: false
class: center middle

![:scale 80%](../../img/joe-caione-qO-PIF84Vxg-unsplash.jpg)

???

Photo by <a href="https://unsplash.com/@joeyc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Joe Caione</a> on <a href="https://unsplash.com/s/photos/happy-puppy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>

---

layout: true

# Shallow Morphological Processing

---

- English L1 and L2 speakers (L2 speakers are native speakers of Cantonese).

- Lexical decision task (Word vs Non-Word).

- Target: *unkindness* ([[un]-[kind]]-ness).

- Primes: *prolong* (Unrelated), *unkind* (Constituent), *kindness* (Non-Constituent).

---

```{r shallow-2}
shallow <- shallow %>%
  mutate(
    accuracy = factor(accuracy, levels = c("incorrect", "correct"))
  )

shallow %>% select(ID, accuracy, Relation_type)
```

---

layout: false

# This doesn't work!

```{r shallow, eval=FALSE, echo=TRUE}
shallow_lm <- lm(accuracy ~ Word_Nonword, data = shallow)
```

```
Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
    contrasts can be applied only to factors with 2 or more levels
```

--

<br>

What is the difference between this model and the models we have run so far?

(**NOTE:** the error message is absolutely NOT helpful!)

--

<br>

**Hint:** Compare the the outcome variables.

---

class: center middle inverse

# PROBABILITY DISTRIBUTIONS

---

class: middle

.pull-left[
![](../../img/06-rainy-umbrella-humid.svg)
]

.pull-right[
![](../../img/01-sunny.svg)
]

???

We are faced every day with probabilities. Just think about the weather forecast.

We say things like t"here is a 70% prob that it will rain today". In this sense, probability is the probability of an event occurring.

But what about more complex situations that are not a flip-of-coin kinda situation? For example what about rolling two dice?

Here is where probability distributions come in.

---

# Grubabilities

&nbsp;

.center[
![:scale 90%](../../img/grubabilities.png)
]

???

A probability distribution is a list of values and their corresponding probability.

---

# Discrete and continuous

.center[
![:scale 80%](../../img/discr-cont-probs.png)
]

???

Remember we talked about continuous and discrete variables?

This distinction is helpful not only when deciding which type of plot to use, but also which type of linear model to use!

Or more specifically, which probability distribution to use for the outcome variable. This week's classes will be about this!

Depending on the nature of the values a variable can take, there are 2 types of probs.



---

# Discrete probability distributions

.center[
![:scale 60%](../../img/dice.png)
]

???


A discrete probability distributions is like counting how many ways you can get a particular value.

For example, if you roll a white and a black dice, there are 3 ways to get a 4 or a 10, but 6 ways to get a 7.

---

layout: true

# Continuous probability distributions

---

```{r cont-p, echo=FALSE}
x <- seq(0, 400)
ggplot() +
  aes(x = x, y = dnorm(x, 200, 50)) +
  geom_line(size = 2) +
  labs(
    x = "f0 (Hz)", y = "Density",
    title = "Gaussian distribution",
    subtitle = expression(paste(mu, "=200", ", ", sigma, "=50"))
  )
```

???

With continuous probabilities we cannot make a list of all the possible values (0.0, 0.00, 0.000, 0.0001...), because there is an infinite number of possible values. So we cannot assign a probability to a specific value.

Instead, we assign probabilities to a range of values.

---

```{r cont-p-2, echo=FALSE}
x <- seq(0, 400)
y = dnorm(x, 200, 50)
ggplot() +
  aes(x, y) +
  geom_ribbon(aes(x = ifelse(x < 160.0001 , x, NA), ymin = 0, ymax = y), alpha = 0.5, fill = "#FFA70B") +
  geom_line(size = 2) +
  scale_x_continuous(breaks = c(0, 100, 160, 200, 300, 400)) +
  labs(
    x = "f0 (Hz)", y = "Density",
    title = "Gaussian distribution",
    subtitle = expression(paste(mu, "=200", ", ", sigma, "=50"))
  )
```

???

In this example, we want to know the probability of observing an f0 value between 0 and 160 Hz, assuming the probability distribution represented in the graph.

We simply calculate the area under the curve between those two values (note that the total area under the curve is 1).

The probability of f0 being less than 160 Hz is `r round(pnorm(160, 200, 50), digits = 3)`.

---

```{r cont-p-3, echo=FALSE}
x <- seq(0, 400)
y = dnorm(x, 200, 50)
ggplot() +
  aes(x, y) +
  geom_ribbon(aes(x = ifelse(x > 219.999 , x, NA), ymin = 0, ymax = y), alpha = 0.5, fill = "#FFA70B") +
  geom_line(size = 2) +
  scale_x_continuous(breaks = c(0, 100, 200, 220, 300, 400)) +
  labs(
    x = "f0 (Hz)", y = "Density",
    title = "Gaussian distribution",
    subtitle = expression(paste(mu, "=200", ", ", sigma, "=50"))
  )
```

???

The probability of f0 being greater than 220 Hz is `r round(pnorm(220, 200, 50, lower.tail = FALSE), digits = 3)`.

---

```{r cont-p-4, echo=FALSE}
x <- seq(0, 400)
y = dnorm(x, 200, 50)
ggplot() +
  aes(x, y) +
  geom_ribbon(aes(x = ifelse(x > 119.999 & x < 210.001 , x, NA), ymin = 0, ymax = y), alpha = 0.5, fill = "#FFA70B") +
  geom_line(size = 2) +
  scale_x_continuous(breaks = c(0, 100, 120, 200, 210, 300, 400)) +
  labs(
    x = "f0 (Hz)", y = "Density",
    title = "Gaussian distribution",
    subtitle = expression(paste(mu, "=200", ", ", sigma, "=50"))
  )
```

???

The probability of f0 being between 120 and 210 Hz is `r round(pnorm(210, 200, 50) - pnorm(120, 200, 50), digits = 3)`

---

But how do we describe probability distributions in a succinct way?

We can't make a list of all values and probabilities, especially for continuous probabilities.

--

.bg-washed-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[

Instead, we specify the value of the **parameters that describe the distribution**.

]

---

layout: false
class: center middle

.f1.link.dim.br3.ph3.pv2.mb2.dib.white.bg-purple[
[.white[Web App: Probability distributions]](https://seeing-theory.brown.edu/probability-distributions/index.html#section2)
]

<!-- <iframe src="https://seeing-theory.brown.edu/probability-distributions/index.html#section2" style="border:none;" width="100%" height="100%"> -->

---
class: middle

<span style="font-size:3.5em;">$$y_i \sim Normal(\mu, \sigma)$$</span>

???

Let's look at some formulae.

This is the formula of a variable $y_1$ that is distributed according to (~) a Normal probability distribution.

As we have seen in the example above, a Normal distribution can be described with two parameters: the mean and the standard deviation.

---

class: middle

<span style="font-size:3.5em;">$$\text{f0}_i \sim Normal(200, 50)$$</span>

???

Remember the example above of a Gaussian/Normal distribution of f0?

We can describe that distribution with this formula (much easier than listing all the values and their probability).

---

class: center middle inverse

# Think about the probability distribution of the outcome variable

---

# Common probability distributions


.pull-left[
**Continuous outcome variable**

* The variable can take on *any positive and negative real number, including 0*: **Gaussian** (aka normal) distribution.

    * There are very few truly Gaussian variables, although in some cases one can speak of "approximate" or "assumed" normality.
    
    * This distribution family is fitted by default in `lm(...)`.
]

--

.pull-right[
**Discrete outcome variable**

* The variable is *dichotomous*, i.e. it can take one of two levels: **Bernoulli** distribution.
  * Categorical outcome variables like yes/no, correct/incorrect, voiced/voiceless, follow this distribution.
  
  * This family is fitted when you run `glm(..., family = binomial)`, aka "logistic regression" or "binomial regression".

* The variable is *counts*: **Poisson** distribution.
  * Counts of words, segments, gestures, f0 peaks, ...
  
  * This family can be fitted with `glm(..., family = poisson)`.
]

???

Note that `glm()` stands for Generalised Linear Model.

It's called "generalised" because the maths behind it generalises the use of linear models with the Gaussian distribution family to other distribution families.

But from a practical point of view, these are all linear models, whether you fit them with `lm()` or `glm()`.

---

# So far we used Gaussian distributions

`lm()` uses a Gaussian distribution by default.

--

But most variables are not Gaussian (in fact, there aren't any truly Gaussian variables, so much for the "normal" distribution).

---

class: center middle inverse

# DICHOTOMOUS VARIABLES: ACCURACY

---

layout: true

# Dichotomous variables: Accuracy

---

```{r shallow-3}
shallow %>% select(ID, accuracy, Relation_type)
```

---

```{r shallow-plot}
shallow %>%
  ggplot(aes(accuracy, fill = accuracy)) +
  geom_bar()
```

???

You can clearly see that `"correct"` is much more frequent than `"incorrect"`.

In other words, the probability of getting `"correct"` is greater than the probability of getting `"incorrect"`.

---

```{r shallow-plot-1}
shallow %>%
  ggplot(aes(Relation_type, fill = accuracy)) +
  geom_bar()
```

???

What happens if we separate by `Releation_type`?

You can still see that `"correct"` is more frequent than `"incorrect"` in all relation types, but since each type has a different number of observations it is not easy to compare *across* types.

---

```{r shallow-plot-2}
shallow %>%
  ggplot(aes(Relation_type, fill = accuracy)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion")
```

???

In this plot, we use `position = "fill"` to plot *proportions* rather than raw counts.

Now you can see that, proportionally, there are more correct responses in `Constituent` vs `NonConstituent` and that `Unrelated` is sorta mid-way through the other two.

---

.bg-washed-blue.b--dark-blue.ba.bw2.br3.shadow-5.ph4.mt5[

When the outcome variable is **dichotomous**, we need to estimate the **probability** of getting either level in the variable.

]

--

In the `shallow` data:

- `accuracy` is dichotomous with levels `"incorrect"` and `"correct"`
- `Relation_type` is discrete with three levels: `Unrelated`, `Constituent`, `NonConstituent`.

So, we want to know (i.e. estimate) the probability of getting a `"correct"` response depending on `Relation_type`.

We need to use `glm()` and `family = binomial()`!

---

```{r shallow-lm-1}
shallow_lm_1 <- glm(accuracy ~ Relation_type, data = shallow, family = binomial())

summary(shallow_lm_1)
```

???

Here's the summary of `shallow_lm_1`.

Do you notice something weird?

---

layout: false
class: middle center inverse

# What is the unit of the estimates?!

???

Should be probabilities, because we are estimating probabilities.

But those cannot be probabilities, because probabilities are between 0 and 1.

---

class: middle center

# Probabilities as log-odds

???

Linear models cannot work with probabilities! 

So we need to transform probabilities into something the model can work with.

And that something is log-odds!

---

layout: true

# Probabilities as log-odds

---

```{r p-log-odds, warning=FALSE}
dots <- tibble(
  p = seq(0.1, 0.9, by = 0.1),
  log_odds = qlogis(p)
)

tibble(
  p = seq(0, 1, by = 0.001),
  log_odds = qlogis(p)
) %>%
  ggplot(aes(log_odds, p)) +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  geom_hline(yintercept = 0, colour = "#8856a7", size = 1) +
  geom_hline(yintercept = 1, colour = "#8856a7", size = 1) +
  geom_vline(xintercept = 0, alpha = 0.5) +
  geom_line(size = 2) +
  # geom_point(data = dots, size = 4) +
  geom_point(x = 0, y = 0.5, colour = "#8856a7", size = 4) +
  scale_x_continuous(breaks = seq(-6, 6, by = 1), minor_breaks = NULL, limits = c(-6, 6)) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = NULL) +
  labs(
    title = "Correspondence between log-odds and probabilities",
    x = "log-odds",
    y = "probability"
  )
```

---

To transform log-odds to probabilities you can use the `plogis()` function!

```{r plogis, echo=TRUE}
# What is the probability at -1 log-odds?
plogis(-1)

# What is the probability at 0 log-odds?
plogis(0)

# What is the probability at 1 log-odds?
plogis(1)
```

--

<br>

Now try different numbers with `plogis()`.

---

layout: false
layout: true

# Dichotomous variables: Accuracy

---

```{r shallow-lm-1-3}
summary(shallow_lm_1)
```

---

```{r sha-lm-sum-1}
tidy(shallow_lm_1, exponentiate = FALSE)
```

--

- `Intercept`
  
  - log-odds = 1.4684
  - `plogis(1.4684)` = `r round(plogis(1.4684), 2)`
  - i.e. 81% probability of getting `"correct"` *when* `Relation_type` is `"Unrelated"`.

--

- What about `Relation_typeConstituent` and `Relation_typeNonConstituent`?

<br>

.f3[As with the estimates of discrete predictors in `lm()` these tell you the difference between `Intercept` and the predictor level, **but in log-odds** rather than probabilities.]

---

```{r sha-lm-sum-2}
tidy(shallow_lm_1, exponentiate = FALSE)
```

We need to **add** the estimate to the intercept to calculate probabilities!

- `Relation_typeConstituent`

  - log-odds = 0.4485.
  - `plogis(1.4684 + 0.4485)` = `r round(plogis(1.4684 + 0.4485), 2)`
  - i.e. when `Relation_type` is `Constituent`, we go from 81% to 87% probability of getting `"correct"`.
  - In other words, there is a probability increase of about 6%.
  
--

- `Relation_typeNonConstituent`

  - log-odds = -0.3492.
  - `plogis(1.4684 + -0.3492)` = `r round(plogis(1.4684 + -0.3492), 2)`
  - i.e. when `Relation_type` is `NonConstituent`, we go from 81% to 75% probability of getting `"correct"`.
  - In other words, there is a probability decrease of about 6%.

---

```{r shallow-lm-1-predics, echo=TRUE}
ggpredict(shallow_lm_1, terms = "Relation_type")
```

---

```{r shallow-lm-1-plot}
ggpredict(shallow_lm_1, terms = "Relation_type") %>%
  plot()
```


---

```{r shallow-lm-2}
shallow_lm_2 <- glm(accuracy ~ Relation_type + Group, data = shallow, family = binomial())

summary(shallow_lm_2)
```


---

```{r shallow-lm-2-plot}
ggpredict(shallow_lm_2, terms = c("Relation_type", "Group"))
```

---

```{r shallow-lm-2-pred}
ggpredict(shallow_lm_2, terms = c("Group", "Relation_type")) %>%
  plot()
```

---

layout: false
class: center middle inverse

# COUNTS: CONTINGENT TALKS

---

layout: true

# Counts: Contingent talks

---

```{r gest}
gestures %>% select(dyad, background, count, ct)
```


---

```{r gest-plot, warning=FALSE}
gestures %>%
  ggplot(aes(count, ct)) +
  geom_point() +
  labs(
    x = "Gesture count",
    y = "Contingent talk count"
  )
```


---

```{r ct-2}
ct_lm_2 <- glm(ct ~ count, data = gestures, family = poisson())

summary(ct_lm_2)
```

---

layout: false
class: middle center inverse

# What is the unit of the estimates?!

???

Should be counts, because we are estimating number of contingent talks.

But those cannot be counts, because counts are discrete and cannot be negative.

---

class: middle center

# Counts as log-odds

???

Linear models don't work well with counts (because they are discrete and cannot be negative)!

So we need to transform counts into something the model can work with.

And that something is (again) log-odds! How convenient!

---

layout: true

# Counts as log-odds

---

To transform log-odds into counts, you can use the `exp()` function.

```{r log-exp, echo=TRUE}
exp(-1)

exp(0)

exp(1)
```

--

<br>

Note that `log()` is the inverse of `exp()`. For example: `log(1)` = 0 and `exp(0)` = 1.

---

layout: false
layout: true

# Counts: Contingent talks

---

```{r ct-2-1}
tidy(ct_lm_2, exponentiate = FALSE)
```

--

- `Intercept`
  
  - log-odds = -1.9519
  - `exp(-1.9519)` = `r round(exp(-1.9519), 2)`.
  - i.e.*when* gesture `count` is `0`, there are between 0 and 1 contingent talks.

--

- What about gesture `count`?

<br>

.f4[As with the estimates of continuous predictors in `lm()` these tell you the difference in `Intercept` when `count` goes from 0 to 1, **but in log-odds** rather than counts.]

--

.f3[When dealing with counts, we normally talk about effects as **rate of change**, aka **odd ratios**.]

???

So we need to transform log-odds into odd ratios (aka simply odds).

How do we do that? Easy! We use the `exp()` function.

So `exp()` converts log-odds into counts and into odd ratios.

---

```{r log-odds}
tibble(
  log_odds = seq(-2, 2, by = 0.001),
  odds = exp(log_odds)
) %>%
  ggplot(aes(log_odds, odds)) +
  geom_vline(xintercept = 0, alpha = 0.5) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  geom_hline(yintercept = 0, size = 1, colour = "#8856a7") +
  geom_line(size = 2) +
  geom_point(x = 0, y = 1, size = 4) +
  scale_y_continuous(breaks = seq(0, 6, by = 1), minor_breaks = NULL) +
  labs(
    title = "Correspondence between log-odds and odds",
    subtitle = "odds = exp(log-odds)",
    x = "log-odds",
    y = "odds"
  )
```

???

When the odds are 1, then it means that there is no change.

Think about this this way:

If you start off with £5 and every day your savings change by a factor of 1, then every day you will still have those £5.
Because `5 * 1 = 5`.

But if every day your savings increase by a factor of 1.5, then after the first day you have £7.5 (`5 * 1.5 = 7.5`), after the second day you have £11.25 (`7.5 * 1.5 = 11.25`), and so on.

You see that the rate of change is always the same (1.5, or 50%), but the absolute change depends on the day's savings:

- first day: +2.5
- second day: +3.75
- third day: +5.62
- ...

---

```{r ct-2-2}
tidy(ct_lm_2, exponentiate = FALSE)
```

- `count`

  - log-odds = 0.133939.
  - `exp(0.133939)` = `r round(exp(0.133939), 2)`
  - i.e. for each unit increase of `count`, coontingent talks increase by a factor of 1.14.
  - In other words, for every one extra gesture, there is a 14% increase in contingent talks.



---

```{r ct-2-plot}
ggpredict(ct_lm_2, terms = c("count [0:30 by=0.1]")) %>%
  plot()
```


---

```{r ct-3}
ct_lm_3 <- glm(ct ~ count + background, data = gestures, family = poisson())

summary(ct_lm_3)
```

---

```{r ct-lm-3-coefs, echo=TRUE}
tidy(ct_lm_3, exponentiate = FALSE)
```

---

```{r ct-lm-3-coefs-exp, echo=TRUE}
tidy(ct_lm_3, exponentiate = TRUE)
```


---

```{r ct-3-plot}
ggpredict(ct_lm_3, terms = c("count [0:30 by=0.1]", "background")) %>%
  plot()
```

???

- 10 mo infants perform on average 10 iconic gestures per day. At 11 mo, the number of gestures increases by a factor of 1.6. At 12 mo, there is a further increase by a factor of 1.2.

--

  - Calculate the average number of gestures per day at 12 months based on the 10 month average (10 gestures).

--

- The average number of errors L2 learners make decreases by a factor of 0.2 every year and a half.

--

  - Calculate the average number of errors after 6 years, assuming 265 errors at year 1.

