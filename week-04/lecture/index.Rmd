---
title: "Statistics and Quantitative Methods (S1)"
subtitle: "Week 4"
author: "Dr Stefano Coretta"
institute: "University of Edinburgh"
date: "2022/10/11"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css:
      - xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "../../assets/macros.js"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, dpi = 300, out.height = "500px", fig.align = "center")
knitr::opts_knit$set(root.dir = here::here())
options(htmltools.dir.version = FALSE)
library(tidyverse)
theme_set(theme_light())
library(magrittr)
library(ggeffects)
library(xaringanExtra)
use_xaringan_extra(c("panelset", "tachyons", "freezeframe"))
options(ggplot2.discrete.fill = RColorBrewer::brewer.pal(5, "Dark2"))
options(ggplot2.discrete.colour = RColorBrewer::brewer.pal(5, "Dark2"))
options(show.signif.stars = FALSE)

library(sqmf)
data("dur_ita_pol")
dur_ita_pol %<>% drop_na(f1, v1_duration)
dur_ita <- filter(dur_ita_pol, language == "Italian")
data("ita_egg")
data("polite")
data("mald_1_1")

mald_1_1 <- mald_1_1 %>%
  mutate(
    Lev_cat = case_when(
      PhonLev < 6.252039 ~ "low",
      PhonLev > 7.323694 ~ "high",
      TRUE ~ "mid"
    ),
    Lev_cat = factor(Lev_cat, levels = c("low", "mid", "high"))
  )

set.seed(8788)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  text_font_google = google_font("Lato", "400", "700", "400i", "700i"),
  header_font_google = google_font("Montserrat", "400", "700", "400i", "700i"),
  code_font_google = google_font("Source Code Pro", "400", "700")
)
```

layout: true

# Summary of last week

---

The data:

```{r dur-ita-pol}
dur_ita_pol %>%
  select(speaker, word, vowel, v1_duration, f1)
```


---

```{r dur-f1}
ggplot(dur_ita_pol, aes(f1, v1_duration)) +
  geom_point(size = 1, alpha = 0.5) +
  geom_smooth(method = "lm")
```

---

```{r dur-f1-summary}
dur_f1_lm <- lm(v1_duration ~ f1, data = dur_ita_pol)

summary(dur_f1_lm)
```

---

We fitted a linear model to vowel duration with first formant (F1) as the only predictor.

According to the model, the mean vowel duration is 70 ms (SE = 3) when F1 is 0 Hz.
For each unit increase of F1, vowels get 0.06 ms longer (SE = 0.005).
In other words, for each 1 Hz increase in F1, there is a 0.06 ms increase in vowel duration.

This means that when F1 increases by 100 Hz, vowels are 6 ms longer.

---

- We can fit linear models with the `lm()` function.
  
  - `lm(outcome ~ predictor, data = your_data)`.
  
- The model `summary()` provides you with the estimates of the model and their standard errors.

- Interpret the estimated coefficients with care!

--

- So far we used **continuous** predictors.

  - `RT ~ PhonLev`: Reaction times as a function of Levenshtein distance.
  - `inmn ~ articulation_rate`: Mean intensity as a function of articulation rate.
  - `v2_duration ~ c2_clos_duration`: Vowel duration as a function of closure duration.

---

.f1[
$$\text{RT} = \beta_0 + \beta_1\text{PhonLev}$$
]

<br>

--

.f2[
$$\beta_0 = \text{intercept}$$
$$\beta_1 = \text{coefficient for PhonLev}$$
]

--

<br>

We know `RT` and `PhonLev` and we need to .green[estimate] $\beta_0$ and $\beta_1$.


---

layout: false
class: center middle inverse

# What if we have *categorical* predictors?

Like group, condition, gender, L1/L2, treatment, place of articulation...

---

layout: true

# Categorical predictors

---

The data:

```{r mald}
mald_1_1 %>%
  select(Subject, RT, IsWord)
```


---

```{r rt-plot}
mald_1_1 %>%
  filter(RT_log > 6) %>%
  ggplot(aes(IsWord, RT)) +
  geom_violin(aes(fill = IsWord), width = 0.5) +
  geom_jitter(size = 1, alpha = 0.1, width = 0.1) +
  geom_boxplot(width = 0.025) +
  scale_y_continuous(breaks = seq(0, 3000, by = 500)) +
  theme(legend.position = "bottom")
```

---

.f1[
$$\text{RT} = \beta_0 + \beta_1\text{IsWord}$$
]

<br>

--

.f2[
$$\beta_0 = \text{intercept}$$
$$\beta_1 = \text{coefficient for IsWord}$$
]

--

<br>

We know `RT` and `IsWord` (`TRUE` or `FALSE`) and we need to .green[estimate] $\beta_0$ and $\beta_1$.

---

layout: false
class: bottom
background-image: url(../../img/hannah-grace-fk4tiMlDFF0-unsplash.jpg)
background-size: cover

# .white[But *IsWord* is not numeric!!!]

???

Photo by <a href="https://unsplash.com/@oddityandgrace?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">hannah grace</a> on <a href="https://unsplash.com/s/photos/surprised-puppy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>


---

layout: true

# Coding categorical predictors

---

Technically, linear models only work with numeric variables.

So we can simply **code our categorical predictors using numbers!**

--

Two common types of coding systems:

- **Treatment** coding.

- **Sum** coding. (We'll do this in Week 7!).

--

Note that you don't have to manually code predictors when using `lm()`.
R does that for you! (But it is important to understand the process to understand how to interpret the model output)

???

As with anything else, naming of coding systems is not an established matter and the same coding can have different names, and vice versa the same name could refer to different systems.

For an excellent overview, see <https://debruine.github.io/faux/articles/contrasts.html>.


---

**Treatment** coding uses `0`s and `1`s to code categorical predictors.

<br>

.f3[
|     | IsWord    |
| --- | -------: |
| TRUE  | 0        |
| FALSE | 1        |
]

---

```{r mald-2}
mald_1_1 %>%
  mutate(IsWord_coded = ifelse(IsWord == "TRUE", 0, 1)) %>%
  select(Subject, Item, RT, IsWord, IsWord_coded)
```

**NOTE:** You don't have to do the coding of the `IsWord` column yourself! `lm()` does everything for you.

---

.f3[
|     | IsWord    |
| --- | -------: |
| TRUE  | 0        |
| FALSE | 1        |
]

<br>

$$\text{RT} = \beta_0 + \beta_1\text{IsWord}$$

<br>

--

$$\text{RT}(IsWord = TRUE) = \beta_0 + \beta_1 \cdot 0 = \beta_0$$

<br>

--

$$\text{RT}(IsWord = FALSE) = \beta_0 + \beta_1 \cdot 1 =  \beta_0 + \beta_1$$

---

With treatment coding, the first level in the predictor is the **reference level** (here, it is `TRUE`).

The other level is compared against the reference level (`FALSE` vs `TRUE`).

--

<br>

.f3[
$$\text{RT} = \beta_0 + \beta_1\text{IsWord}$$
]

<br>

.f2[
$$\beta_0 = RT_\text{IsWord=T}$$
]

.f2[
$$\beta_1 = RT_{\text{IsWord=F}} - RT_{\text{IsWord=T}}$$
]

???

For now, it is easy because we only have two levels, but we will see an example with three later.

$\beta_0$, aka the *intercept*, corresponds to the mean Reaction Time when `IsWord = TRUE`, because `TRUE` is the reference level of `IsWord`.

$\beta_1$ corresponds to the **DIFFERENCE** between the *intercept* and the mean Reaction Time when `IsWord = FALSE`. In other words, we are comparing RTs when `IsWord = FALSE` vs when `IsWord = TRUE`.

---

```{r rt-lm, echo=TRUE}
isword_lm <- lm(RT ~ IsWord, data = mald_1_1)
summary(isword_lm)
```

???

So, how do we interpret the output? Easy!

- `(Intercept)`: the mean Reaction Times when `IsWord` is `TRUE`.

- `IsWordFalse`:

  - The **effect** of `IsWord` on RTs when `IsWord = FALSE`.
  - The **difference** between RTs when `IsWord = FALSE` and `IsWord = TRUE`.
  - What you **need to add to the `(Intercept)`** to get the mean RT value of `IsWord = FALSE`.

---

```{r rt-lm-plot}
ggpredict(isword_lm, "IsWord") %>%
  plot()
```

???

If you are curious about the inner workings of the coding system, try the following code:


```r
mald_coded <- mald_1_1 %>%
  mutate(
    IsWordFALSE = ifelse(
      IsWord == "TRUE",
      0,
      1)
  )

lm_2 <- lm(RT ~ IsWordFALSE,
          data = mald_coded)
summary(lm_2)
```

---

layout: false
layout: true

# Three-levels

---

The data:

```{r dur-ita}
dur_ita %>%
  select(speaker, word, vowel, v2_duration)
```

---

```{r vdur-plot}
dur_ita %>%
  ggplot(aes(vowel, v2_duration)) +
  geom_violin(aes(fill = vowel), width = 0.5) +
  geom_jitter(size = 1, alpha = 0.1, width = 0.1) +
  geom_boxplot(width = 0.025) +
  theme(legend.position = "bottom")
```


---

How can we code three levels with just `0`s and `1`s?

.f3[
|      | vowel  |
| ---- | -------: |
| a  | 0        |
| o  | 1        |
| u   | ???      |
]

---

Easy! We cheat the system...

.f3[
|      | vowel_o  | vowel_u |
| ---- | -------: | -------: |
| a  | 0        | 0        |
| o  | 1        | 0        |
| u | 0        | 1        |
]

---

```{r dur-ita-1}
dur_ita %>%
  mutate(
    vowel_o = ifelse(vowel == "o", 1, 0),
    vowel_u = ifelse(vowel == "u", 1, 0)
  ) %>% select(speaker, word, vowel, vowel_o, vowel_u)
```

**NOTE:** You don't have to do the coding of the `vowel` column yourself! `lm()` does everything for you.

---

.f3[
|      | vowel_o  | vowel_u |
| ---- | -------: | -------: |
| a  | 0        | 0        |
| o  | 1        | 0        |
| u | 0        | 1        |
]

<br>

$$\text{vdur} = \beta_0 + \beta_1\text{vowel_o} + \beta_2\text{vowel_u}$$

--


<br>

We know `v2_duration` and `vowel` (`a`, `o` or `u`) and we need to .green[estimate] $\beta_0$,  $\beta_1$ and $\beta_2$.


---


$$\text{vdur}(vowel = a) = \beta_0 + \beta_1 \cdot 0 + \beta_2 \cdot 0 = \beta_0$$

<br>

--

$$\text{vdur}(vowel = o) = \beta_0 + \beta_1 \cdot 1 + \beta_2 \cdot 0 = \beta_0 + \beta_1$$

<br>

--

$$\text{vdur}(vowel = u) = \beta_0 + \beta_1 \cdot 0 + \beta_2 \cdot 1 = \beta_0 + \beta_2$$


---

```{r vdur-lm}
vdur_lm <- lm(v2_duration ~ vowel, data = dur_ita)

summary(vdur_lm)
```

---

```{r vdur-lm-plot}
ggpredict(vdur_lm, terms = "vowel") %>% plot()
```

---

```{r vdur-lm-pred}
ggpredict(vdur_lm, terms = "vowel")
```

---

layout: false
layout: true

# Four levels

---

The data:

```{r ita_egg}
ita_egg %>%
  select(speaker, word, vowel, height, voice_duration)
```


---

```{r itaegg-plot}
ita_egg %>%
  ggplot(aes(height, voice_duration)) +
  geom_violin(aes(fill = height), width = 0.5) +
  geom_jitter(size = 1, alpha = 0.1, width = 0.1) +
  geom_boxplot(width = 0.025) +
  theme(legend.position = "bottom")
```

---

You know the drill...

.f3[
|          | height_midlow | height_midhigh | height_high |
|----------| :----------------: | :-----------------: | :-------------: |
| low      | 0              | 0               | 0           |
| mid-low  | 1              | 0               | 0           |
| mid-high | 0              | 1               | 0           |
| high     | 0              | 0               | 1           |
]

---

```{r ita-egg-2}
ita_egg %>%
  mutate(
    height_midlow = ifelse(height == "mid-low", 1, 0),
    height_midhigh = ifelse(height == "mid-high", 1, 0),
    height_high = ifelse(height == "high", 1, 0)
  ) %>%
  select(speaker, word, vowel, height, height_midlow, height_midhigh, height_high)
```

**NOTE:** You don't have to do the coding of the `height` column yourself! `lm()` does everything for you.

---

.f3[
|          | height_midlow | height_midhigh | height_high |
|----------| :----------------: | :-----------------: | :-------------: |
| low      | 0              | 0               | 0           |
| mid-low  | 1              | 0               | 0           |
| mid-high | 0              | 1               | 0           |
| high     | 0              | 0               | 1           |
]

<br>

$$\text{voicedur} = \beta_0 + \beta_1\text{height_midlow} + \beta_2\text{height_midhigh} + \beta_3\text{height_high}$$

--


<br>

We know `voice_duration` and `height` (`low`, `mid-low`, `mid-high` or `high`) and we need to .green[estimate] $\beta_0$, $\beta_1$, $\beta_2$ and $\beta_3$.

---

```{r itaegg-lm}
voicedur_lm <- lm(voice_duration ~ height, data = ita_egg)

summary(voicedur_lm)
```

---

```{r itaegg-lm-plot}
ggpredict(voicedur_lm, terms = "height") %>% plot()
```

---

```{r itaegg-lm-pred}
ggpredict(voicedur_lm, terms = "height")
```




---

layout: false
layout: true

# Two categorical predictors

---

The data:

```{r dur-ita-2}
dur_ita %>%
  select(speaker, word, vowel, c2_phonation, v2_duration)
```


---

```{r vdur-lm-2, echo=TRUE}
vdur_lm_2 <- lm(v2_duration ~ vowel + c2_phonation, data = dur_ita)

summary(vdur_lm_2)
```

---

```{r vdur-lm-2-pred, echo=TRUE}
ggpredict(vdur_lm_2)
```

---

```{r vdur-lm-2-pred-2, echo=TRUE}
ggpredict(vdur_lm_2, terms = c("vowel", "c2_phonation"))
```

---

```{r vdur-lm-2-plot-1}
ggpredict(vdur_lm_2, terms = c("vowel", "c2_phonation")) %>% plot()
```

---

```{r vdur-lm-2-plot-2}
ggpredict(vdur_lm_2, terms = c("c2_phonation", "vowel")) %>% plot()
```

---

layout: false

# Summary


.pull-left[
* The simplest form of a linear model is the following:

  * $y = \beta_0 + \beta_1 x$, in R syntax `lm(y ~ x)`.
  
* We want to estimate the $\beta_n$ .orange[**coefficients**].

* When `x` and `y` are **both continuous variables**, all good!

  * $\beta_0$ is the intercept.
  * $\beta_1$ is the slope.

* In other words:
  
  * $\beta_0$ is the mean value of `y` when `x` is 0.
  * $\beta_1$ is what you have to add (or subtract) to $\beta_0$ when `x` goes from 0 to 1.
]

--

.pull-right[
* When `y` is continuous but `x` is a **discrete variable**:

  * We code discrete predictors as numbers with treatment coding (i.e. with `0`s and `1`s).
  
  * $\beta_0$ is the mean value of `y` when `x` is at the reference level.
  
  * $\beta_1$ is what you have to add (or subtract) to $\beta_0$ when `x` is at the other level.
  
  * and so on for each $\beta_n$...
]
