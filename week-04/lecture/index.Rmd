---
title: "Statistics and Quantitative Methods (S1)"
subtitle: "Week 4"
author: "Dr Stefano Coretta"
institute: "University of Edinburgh"
date: "2022/09/01"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css:
      - xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "../../assets/macros.js"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, dpi = 300, out.height = "500px", fig.align = "center")
knitr::opts_knit$set(root.dir = here::here())
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(magrittr)
theme_set(theme_light())
library(xaringanExtra)
use_xaringan_extra(c("panelset", "tachyons", "freezeframe"))
library(sqmf)
data("dur_ita_pol")
dur_ita_pol %<>% drop_na(f1, v1_duration)
data("polite")
data("mald_1_1")
mald_1_1 <- mald_1_1 %>%
  mutate(
    Lev_cat = case_when(
      PhonLev < 6.252039 ~ "low",
      PhonLev > 7.323694 ~ "high",
      TRUE ~ "mid"
    ),
    Lev_cat = factor(Lev_cat, levels = c("low", "mid", "high"))
  )
set.seed(8788)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  text_font_google = google_font("Lato", "400", "700", "400i", "700i"),
  header_font_google = google_font("Montserrat", "400", "700", "400i", "700i"),
  code_font_google = google_font("Source Code Pro", "400", "700")
)
```

layout: true

# Linear model: the basics

---

```{r line-model}
ggplot(dur_ita_pol, aes(f1, v1_duration)) +
  geom_point(size = 1, alpha = 0.5) +
  geom_smooth(method = "lm")
```

---

```{r}
dur_f1_lm <- lm(v1_duration ~ f1, data = dur_ita_pol)

summary(dur_f1_lm)
```

---

layout: false

# Summary of last week

- ...

- So far we used **continuous** predictors.

  - ...

---

class: center middle inverse

# What if we have **categorical** predictors?

Like group, condition, gender, ...

---

# Categorical predictors

```{r categorical}
mald_1_1 %>%
  filter(RT_log > 6) %>%
  ggplot(aes(IsWord, RT)) +
  geom_violin(aes(fill = IsWord), width = 0.5) +
  geom_jitter(size = 1, alpha = 0.1, width = 0.1) +
  geom_boxplot(width = 0.025) +
  scale_y_continuous(breaks = seq(0, 3000, by = 500)) +
  theme(legend.position = "bottom")
```

---

# Categorical predictors


.f1[
$$\text{RT} = \beta_0 + \beta_1\text{IsWord}$$
]

<br>

--

.f2[
$$\beta_0 = \text{intercept}$$
$$\beta_1 = \text{coefficient for IsWord}$$
]

--

<br>

We know `RT` and `IsWord` (`TRUE` or `FALSE`) and we need to .green[estimate] $\beta_0$ and $\beta_1$.

---

class: bottom
background-image: url(../img/hannah-grace-fk4tiMlDFF0-unsplash.jpg)
background-size: cover

# .white[But *IsWord* is not numeric!!!]

???

Photo by <a href="https://unsplash.com/@oddityandgrace?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">hannah grace</a> on <a href="https://unsplash.com/s/photos/surprised-puppy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>


---

# Dummy coding

.f3[
|     | IsWord    |
| --- | -------: |
| TRUE  | 0        |
| FALSE | 1        |
]

<br>

$$\text{RT} = \beta_0 + \beta_1\text{IsWord}$$

<br>

--

$$\text{RT}(IsWord = TRUE) = \beta_0 + \beta_1 \cdot 0 = \beta_0$$

<br>

--

$$\text{RT}(IsWord = FALSE) = \beta_0 + \beta_1 \cdot 1 =  \beta_0 + \beta_1$$

---

# Categorical predictors

```{r lm-group, echo=TRUE}
isword_lm <- lm(RT ~ IsWord, data = mald_1_1)
summary(isword_lm)
```


---

```{r}
mald_1_1 %>%
  ggplot(aes(Lev_cat, RT)) +
  geom_violin(aes(fill = Lev_cat), width = 0.5) +
  geom_jitter(size = 1, alpha = 0.1, width = 0.1) +
  geom_boxplot(width = 0.025) +
  scale_y_continuous(breaks = seq(0, 3000, by = 500)) +
  theme(legend.position = "bottom")
```


---

# Dummy coding

.f3[
|      | Lev_cat  |
| ---- | -------: |
| low  | 0        |
| mid  | 1        |
| high | ???      |
]

---

# Dummy coding

.f3[
|      | Lev_cat_mid  | Lev_cat_high |
| ---- | -------: | -------: |
| low  | 0        | 0        |
| mid  | 1        | 0        |
| high | 0        | 1        |
]

<br>

$$\text{RT} = \beta_0 + \beta_1\text{Lev_cat_mid} + \beta_2\text{Lev_cat_high}$$

--


<br>

We know `RT` and `Lev_cat` (`low`, `mid` or `high`) and we need to .green[estimate] $\beta_0$,  $\beta_1$ and $\beta_2$.


---


--

$$\text{RT}(Lev_cat = low) = \beta_0 + \beta_1 \cdot 0 + \beta_2 \cdot 0 = \beta_0$$

<br>

--

$$\text{RT}(Lev_cat = mid) = \beta_0 + \beta_1 \cdot 1 + \beta_2 \cdot 0 = \beta_0 + \beta_1$$

<br>

--

$$\text{RT}(Lev_cat = high) = \beta_0 + \beta_1 \cdot 0 + \beta_2 \cdot 1 = \beta_0 + \beta_2$$


---

```{r}
cat_3 <- lm(RT ~ Lev_cat, data = mald_1_1)

summary(cat_3)
```



---

# Summary

* The simplest .orange[**linear model**] is a straight line.

$$y = \beta_0 + \beta_1 x$$

* We want to estimate the $\beta_n$ .orange[**coefficients**].

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_4 x_1 x_2 + \beta_3 x_3$$

* Categorical predictors are .orange[dummy coded].

