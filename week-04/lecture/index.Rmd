---
title: "Statistics and Quantitative Methods (S1)"
subtitle: "Week 4"
author: "Dr Stefano Coretta"
institute: "University of Edinburgh"
date: "2022/09/01"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css:
      - xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "../../assets/macros.js"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, dpi = 300, out.height = "500px", fig.align = "center")
knitr::opts_knit$set(root.dir = here::here())
options(htmltools.dir.version = FALSE)
library(tidyverse)
theme_set(theme_light())
library(magrittr)
library(ggeffects)
library(xaringanExtra)
use_xaringan_extra(c("panelset", "tachyons", "freezeframe"))

library(sqmf)
data("dur_ita_pol")
dur_ita_pol %<>% drop_na(f1, v1_duration)
data("polite")
data("mald_1_1")

mald_1_1 <- mald_1_1 %>%
  mutate(
    Lev_cat = case_when(
      PhonLev < 6.252039 ~ "low",
      PhonLev > 7.323694 ~ "high",
      TRUE ~ "mid"
    ),
    Lev_cat = factor(Lev_cat, levels = c("low", "mid", "high"))
  )

set.seed(8788)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#23395b",
  text_font_google = google_font("Lato", "400", "700", "400i", "700i"),
  header_font_google = google_font("Montserrat", "400", "700", "400i", "700i"),
  code_font_google = google_font("Source Code Pro", "400", "700")
)
```

layout: true

# Summary of last week

---

```{r dur-f1}
ggplot(dur_ita_pol, aes(f1, v1_duration)) +
  geom_point(size = 1, alpha = 0.5) +
  geom_smooth(method = "lm")
```

---

```{r dur-f1-summary}
dur_f1_lm <- lm(v1_duration ~ f1, data = dur_ita_pol)

summary(dur_f1_lm)
```

---

- We can fit linear models with the `lm()` function.
  
  - `lm(outcome ~ predictor, data = your_data)`.
  
- The model `summary()` provides you with the estimates of the model and their standard errors.

--

- So far we used **continuous** predictors.

  - `RT ~ PhonLev`: Reaction times as a function of Levenshtein distance.
  - `inmn ~ articulation_rate`: Mean intensity as a function of articulation rate.
  - `v2_duration ~ c2_clos_duration`: Vowel duration as a function of closure duration.

---

.f1[
$$\text{RT} = \beta_0 + \beta_1\text{PhonLev}$$
]

<br>

--

.f2[
$$\beta_0 = \text{intercept}$$
$$\beta_1 = \text{coefficient for PhonLev}$$
]

--

<br>

We know `RT` and `PhonLev` and we need to .green[estimate] $\beta_0$ and $\beta_1$.


---

layout: false
class: center middle inverse

# What if we have *categorical* predictors?

Like group, condition, gender, L1/L2, treatment, place of articulation...

---

layout: true

# Categorical predictors

---

```{r categorical}
mald_1_1 %>%
  filter(RT_log > 6) %>%
  ggplot(aes(IsWord, RT)) +
  geom_violin(aes(fill = IsWord), width = 0.5) +
  geom_jitter(size = 1, alpha = 0.1, width = 0.1) +
  geom_boxplot(width = 0.025) +
  scale_y_continuous(breaks = seq(0, 3000, by = 500)) +
  theme(legend.position = "bottom")
```

---

.f1[
$$\text{RT} = \beta_0 + \beta_1\text{IsWord}$$
]

<br>

--

.f2[
$$\beta_0 = \text{intercept}$$
$$\beta_1 = \text{coefficient for IsWord}$$
]

--

<br>

We know `RT` and `IsWord` (`TRUE` or `FALSE`) and we need to .green[estimate] $\beta_0$ and $\beta_1$.

---

layout: false
class: bottom
background-image: url(../../img/hannah-grace-fk4tiMlDFF0-unsplash.jpg)
background-size: cover

# .white[But *IsWord* is not numeric!!!]

???

Photo by <a href="https://unsplash.com/@oddityandgrace?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">hannah grace</a> on <a href="https://unsplash.com/s/photos/surprised-puppy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>


---

layout: true

# Coding categorical predictors

---

Technically, linear models only work with numeric variables.

So we can simply **code our categorical predictors using numbers!**

--

Two common types of coding systems:

- **Treatment** coding.

- **Sum** coding.

???

As with anything else, naming of coding systems is not an established matter and the same coding can have different names, and vice versa the same name could refer to different systems.

For an excellent overview, see <https://debruine.github.io/faux/articles/contrasts.html>.


---

**Treatment** coding uses `0`s and `1`s to code categorical predictors.

<br>

.f3[
|     | IsWord    |
| --- | -------: |
| TRUE  | 0        |
| FALSE | 1        |
]

<br>

$$\text{RT} = \beta_0 + \beta_1\text{IsWord}$$

<br>

--

$$\text{RT}(IsWord = TRUE) = \beta_0 + \beta_1 \cdot 0 = \beta_0$$

<br>

--

$$\text{RT}(IsWord = FALSE) = \beta_0 + \beta_1 \cdot 1 =  \beta_0 + \beta_1$$

---

With treatment coding, the first level in the predictor is the **reference level** (here, it is `TRUE`).

The other level is compared agains the reference level (`FALSE` vs `TRUE`).

--

<br>

.f3[
$$\text{RT} = \beta_0 + \beta_1\text{IsWord}$$
]

<br>

.f2[
$$\beta_0 = RT_\text{IsWord=T}$$
]

.f2[
$$\beta_1 = RT_{\text{IsWord=F}} - RT_{\text{IsWord=T}}$$
]

???

For now, it is easy because we only have two levels, but we will see an example with three later.

$\beta_0$, aka the *intercept*, corresponds to the mean Reaction Time when `IsWord = TRUE`, because `TRUE` is the reference level of `IsWord`.

$\beta_1$ corresponds to the **DIFFERENCE** between the *intercept* and the mean Reaction Time when `IsWord = FALSE`. In other words, we are comparing RTs when `IsWord = FALSE` vs when `IsWord = TRUE`.

---

```{r lm-group, echo=TRUE}
isword_lm <- lm(RT ~ IsWord, data = mald_1_1)
summary(isword_lm)
```

???

So, how do we interpret the output? Easy!

- `(Intercept)`: the mean Reaction Times when `IsWord` is `TRUE`.

- `IsWordFalse`:

  - The **effect** of `IsWord` on RTs when `IsWord = FALSE`.
  - The **difference** between RTs when `IsWord = FALSE` and `IsWord = TRUE`.
  - What you **need to add to the `(Intercept)`** to get the mean RT value of `IsWord = FALSE`.

---

```{r}
mald_1_1 %>%
  ggplot(aes(Lev_cat, RT)) +
  geom_violin(aes(fill = Lev_cat), width = 0.5) +
  geom_jitter(size = 1, alpha = 0.1, width = 0.1) +
  geom_boxplot(width = 0.025) +
  scale_y_continuous(breaks = seq(0, 3000, by = 500)) +
  theme(legend.position = "bottom")
```


---

.f3[
|      | Lev_cat  |
| ---- | -------: |
| low  | 0        |
| mid  | 1        |
| high | ???      |
]

---

.f3[
|      | Lev_cat_mid  | Lev_cat_high |
| ---- | -------: | -------: |
| low  | 0        | 0        |
| mid  | 1        | 0        |
| high | 0        | 1        |
]

<br>

$$\text{RT} = \beta_0 + \beta_1\text{Lev_cat_mid} + \beta_2\text{Lev_cat_high}$$

--


<br>

We know `RT` and `Lev_cat` (`low`, `mid` or `high`) and we need to .green[estimate] $\beta_0$,  $\beta_1$ and $\beta_2$.


---


--

$$\text{RT}(Lev_cat = low) = \beta_0 + \beta_1 \cdot 0 + \beta_2 \cdot 0 = \beta_0$$

<br>

--

$$\text{RT}(Lev_cat = mid) = \beta_0 + \beta_1 \cdot 1 + \beta_2 \cdot 0 = \beta_0 + \beta_1$$

<br>

--

$$\text{RT}(Lev_cat = high) = \beta_0 + \beta_1 \cdot 0 + \beta_2 \cdot 1 = \beta_0 + \beta_2$$


---

```{r}
cat_3 <- lm(RT ~ Lev_cat, data = mald_1_1)

summary(cat_3)
```

---

```{r}
ggpredict(cat_3, terms = "Lev_cat") %>% plot()
```

---

```{r}
ggpredict(cat_3, terms = "Lev_cat")
```

---

```{r}
cat_4 <- lm(RT ~ IsWord + Lev_cat, data = mald_1_1)

summary(cat_4)
```

---

```{r}
ggpredict(cat_4)
```

---

```{r}
ggpredict(cat_4, terms = c("IsWord", "Lev_cat"))
```

---

```{r}
ggpredict(cat_4, terms = c("IsWord", "Lev_cat")) %>% plot()
```

---

```{r}
ggpredict(cat_4, terms = c("Lev_cat", "IsWord")) %>% plot()
```

---

layout: false

# Summary

* The simplest .orange[**linear model**] is a straight line.

$$y = \beta_0 + \beta_1 x$$

* We want to estimate the $\beta_n$ .orange[**coefficients**].

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_4 x_1 x_2 + \beta_3 x_3$$

* Categorical predictors are .orange[dummy coded].

